{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "colab_poem_generator_word.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aaronsom/poem-generation/blob/master/colab_poem_generator_word.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-6TEToOvN-b",
        "colab_type": "code",
        "outputId": "ebe9bb0f-b435-4417-a488-f1f23e134a6a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "!git clone https://github.com/Aaronsom/poem-generation\n",
        "%cd poem-generation\n",
        "%mkdir models\n",
        "!wget https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
        "!gunzip GoogleNews-vectors-negative300.bin"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'poem-generation' already exists and is not an empty directory.\n",
            "/content/poem-generation\n",
            "mkdir: cannot create directory ‘models’: File exists\n",
            "--2019-06-01 17:49:55--  https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.134.93\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.134.93|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1647046227 (1.5G) [application/x-gzip]\n",
            "Saving to: ‘GoogleNews-vectors-negative300.bin.gz’\n",
            "\n",
            "GoogleNews-vectors- 100%[===================>]   1.53G  20.0MB/s    in 67s     \n",
            "\n",
            "2019-06-01 17:51:03 (23.3 MB/s) - ‘GoogleNews-vectors-negative300.bin.gz’ saved [1647046227/1647046227]\n",
            "\n",
            "gzip: GoogleNews-vectors-negative300.bin: unknown suffix -- ignored\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFPj1bshvXIC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger\n",
        "import tensorflow.train as optimizer\n",
        "from poem_generator.dataGenerator import TupleDataGenerator\n",
        "import poem_generator.data_prepocessing as dp\n",
        "import poem_generator.embedding as embedding_loader\n",
        "from poem_generator.global_constants import TRAINING_DATA, EMBEDDING_DIMENSION, EMBEDDING_BINARY, MODELS_DICT\n",
        "from poem_generator.transformer import transformer\n",
        "#from poem_generator.PoemCallback import PoemCallback\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.contrib.tpu import keras_to_tpu_model, TPUDistributionStrategy\n",
        "from tensorflow.contrib.cluster_resolver import TPUClusterResolver\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHqUHLzDHkIa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.callbacks import Callback\n",
        "\n",
        "class PoemCallback(Callback):\n",
        "\n",
        "    def __init__(self, poems, seed_length, dictionary, single=True):\n",
        "        super(PoemCallback, self).__init__()\n",
        "        self.poems = poems\n",
        "        self.dictionary = dictionary\n",
        "        self.reverse_dictionary = {dictionary[key]: key for key in dictionary.keys()}\n",
        "        self.seed_length = seed_length\n",
        "        self.single = single\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        for i in range(self.poems):\n",
        "            print(f\"Poem {i+1}/{self.poems}\")\n",
        "            model = self.model.sync_to_cpu()\n",
        "            self.generate_poem(model, self.reverse_dictionary, self.dictionary, self.seed_length, single=self.single)\n",
        "            \n",
        "    def generate_poem(self, model, reverse_dictionary, dictionary, seed_length, dynamic_seed=False, single=False):\n",
        "        poem = \"\"\n",
        "        last_output = \"\"\n",
        "        iterations = 0\n",
        "        seed = np.array([dictionary[START_OF_SEQUENCE_TOKEN]]*seed_length)\n",
        "        already_eol = False  # Sometimes too many eols are generated, this breaks the format\n",
        "        while iterations < 60 and last_output != END_OF_SEQUENCE_TOKEN:\n",
        "            if single:\n",
        "                last_output_dist = model.predict(np.array([seed])).squeeze()\n",
        "            else:\n",
        "                last_output_dist = model.predict(np.array([seed]))[:, -1].squeeze()\n",
        "            last_output_idx = np.random.choice(len(dictionary), 1, p=last_output_dist).item()\n",
        "            last_output = reverse_dictionary[last_output_idx]\n",
        "\n",
        "\n",
        "            iterations += 1\n",
        "\n",
        "            if last_output == END_OF_SEQUENCE_TOKEN or iterations == 60:\n",
        "                if already_eol:\n",
        "                    poem += \"\\n\"\n",
        "                else:\n",
        "                    poem += \"\\n\\n\"\n",
        "            elif last_output == OUT_OF_VOCAB_TOKEN or last_output == PADDING_TOKEN \\\n",
        "                    or last_output == START_OF_SEQUENCE_TOKEN:\n",
        "                iterations -= 1\n",
        "            elif last_output == END_OF_LINE_TOKEN:\n",
        "                if iterations > 1 and not already_eol:\n",
        "                    already_eol = True\n",
        "                    poem += \"\\n\"\n",
        "\n",
        "            else:\n",
        "                already_eol = False\n",
        "                poem += last_output + \" \"\n",
        "            if last_output != OUT_OF_VOCAB_TOKEN and last_output != PADDING_TOKEN:\n",
        "                if not dynamic_seed:\n",
        "                    seed = np.append(seed[1:], last_output_idx)\n",
        "                else:\n",
        "                    seed = np.append(seed, last_output_idx)\n",
        "        print(poem)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWSwF1dx8t_4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def bidirectional_lstm(n, embedding, vocab_len):\n",
        "    model = Sequential([\n",
        "        Embedding(input_dim=vocab_len, output_dim=EMBEDDING_DIMENSION, input_length=n, weights=[embedding]),\n",
        "        Bidirectional(LSTM(512, return_sequences=True)),\n",
        "        Bidirectional(LSTM(512, return_sequences=False)),\n",
        "        Dropout(0.1),\n",
        "        Dense(vocab_len, activation=\"softmax\")\n",
        "    ])\n",
        "    return model\n",
        "  \n",
        "def lstm_rnn(n, embedding, vocab_len):\n",
        "    model = Sequential([\n",
        "        Embedding(input_dim=vocab_len, output_dim=EMBEDDING_DIMENSION, input_length=n, weights=[embedding]),\n",
        "        LSTM(512, return_sequences=True),\n",
        "        LSTM(512, return_sequences=False),\n",
        "        Dropout(0.1),\n",
        "        Dense(vocab_len, activation=\"softmax\")\n",
        "    ])\n",
        "    return model\n",
        "  \n",
        "def mlp(n, embedding, vocab_len):\n",
        "    model = Sequential([\n",
        "        Embedding(input_dim=vocab_len, output_dim=EMBEDDING_DIMENSION, input_length=n, weights=[embedding]),\n",
        "        Flatten(),\n",
        "        Dropout(0.1),\n",
        "        Dense(n*512, activation=\"relu\"),\n",
        "        Dropout(0.1),\n",
        "        Dense(vocab_len,activation=\"softmax\"),\n",
        "    ])\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f28-Yvquvcsh",
        "colab_type": "code",
        "outputId": "32bcdf59-1538-4dda-df95-dd45407840cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1159
        }
      },
      "source": [
        "\n",
        "\n",
        "ns = [5]\n",
        "epochs = 30\n",
        "batch_size = 512\n",
        "max_limit = 25000\n",
        "validation_split = 0.9\n",
        "\n",
        "poems = dp.tokenize_poems(TRAINING_DATA)\n",
        "words = sorted(list(set([token for poem in poems for token in poem])))\n",
        "\n",
        "#Save embedding for generator\n",
        "embedding, dictionary = embedding_loader.get_embedding(words, binary=EMBEDDING_BINARY, limit=max_limit, save=True, file=\"GoogleNews-vectors-negative300.bin\")\n",
        "\n",
        "#model = load_model(MODELS_DICT+\"/5model.hdf5\", custom_objects={\"PositionalEncoding\": PositionalEncoding, \"Attention\": Attention})\n",
        "#model = transformer(ns[0], embedding, len(dictionary), single_out=True, train_embedding=True, input_sequence_length=ns[0], blocks=1, heads=5)\n",
        "#model = bidirectional_lstm(ns[0], embedding, len(dictionary))\n",
        "model = mlp(ns[0], embedding, len(dictionary))\n",
        "#model.summary()\n",
        "tpu_model = keras_to_tpu_model(\n",
        "    model,\n",
        "    strategy=TPUDistributionStrategy(\n",
        "        TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\n",
        "    )\n",
        ")\n",
        "tpu_model.compile(optimizer=optimizer.AdamOptimizer(),\n",
        "            loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "generator = TupleDataGenerator(poems[:int(validation_split*len(poems))], ns, dictionary, 0, batch_size, single=True)\n",
        "validation_generator = TupleDataGenerator(poems[int(validation_split*len(poems)):], ns, dictionary, 0, batch_size, single=True)\n",
        "callbacks = [ModelCheckpoint(MODELS_DICT+\"/model.hdf5\", save_best_only=True),\n",
        "           CSVLogger(MODELS_DICT+\"/log.csv\", append=True, separator=';'), PoemCallback(2, ns[0], dictionary)]\n",
        "tpu_model.fit_generator(\n",
        "  generator, epochs=epochs, callbacks=callbacks, validation_data=validation_generator, workers=4)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            (None, 5)            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, 5, 300)       2662800     input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "positional_encoding_1 (Position (None, 5, 300)       0           embedding_1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout_8 (Dropout)             (None, 5, 300)       0           positional_encoding_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "attention_3 (Attention)         (None, 5, 300)       614400      dropout_8[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_9 (Dropout)             (None, 5, 300)       0           attention_3[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "add_5 (Add)                     (None, 5, 300)       0           dropout_9[0][0]                  \n",
            "                                                                 dropout_8[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_7 (TimeDistrib (None, 5, 300)       1200        add_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_8 (TimeDistrib (None, 5, 300)       90300       time_distributed_7[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "dropout_10 (Dropout)            (None, 5, 300)       0           time_distributed_8[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "add_6 (Add)                     (None, 5, 300)       0           dropout_8[0][0]                  \n",
            "                                                                 dropout_10[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_9 (TimeDistrib (None, 5, 300)       1200        add_6[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_15 (Dropout)            (None, 5, 300)       0           time_distributed_9[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "lambda_1 (Lambda)               (None, 300)          0           dropout_15[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, 8876)         2671676     lambda_1[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 6,041,576\n",
            "Trainable params: 3,377,576\n",
            "Non-trainable params: 2,664,000\n",
            "__________________________________________________________________________________________________\n",
            "INFO:tensorflow:Querying Tensorflow master (grpc://10.88.157.2:8470) for TPU system metadata.\n",
            "INFO:tensorflow:Found TPU system:\n",
            "INFO:tensorflow:*** Num TPU Cores: 8\n",
            "INFO:tensorflow:*** Num TPU Workers: 1\n",
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 3008627379228793816)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 14939066834168397888)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 10479959879007220193)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 7408022553672936033)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 15813026506340347212)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 2863010759329560436)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 15274504238755014854)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 10230841176501591631)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 2979214698172045833)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 6292248463295228262)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 7688213722672729447)\n",
            "WARNING:tensorflow:tpu_model (from tensorflow.contrib.tpu.python.tpu.keras_support) is experimental and may change or be removed at any time, and without warning.\n",
            "Training on 8876 words with 937025 [5]-tuples\n",
            "Training on 8876 words with 115190 [5]-tuples\n",
            "Epoch 1/20\n",
            "INFO:tensorflow:New input shapes; (re-)compiling: mode=train (# of cores 8), [TensorSpec(shape=(64,), dtype=tf.int32, name='core_id_20'), TensorSpec(shape=(64, 5), dtype=tf.float32, name='input_2_10'), TensorSpec(shape=(64, 8876), dtype=tf.float32, name='dense_5_target_10')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for input_2\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 12.415066480636597 secs\n",
            "INFO:tensorflow:Setting weights on TPU model.\n",
            " 374/1313 [=======>......................] - ETA: 3:59 - loss: 6.7698 - acc: 0.1511"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tko91UHXhqgU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir generated"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aS1cuPAN0Rb6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from poem_generator.word_generator import generate_poems\n",
        "n = ns[0]\n",
        "generate_poems(1000, n, \"generated/poems.zip\", MODELS_DICT+\"/model.hdf5\", single=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2PTMsNQ7YQL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "files.download(\"generated/poems.zip\")\n",
        "files.download(\"model/log.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}